import torch
import numpy as np
import logging
import hydra
from srcs.trainer import Trainer
from srcs.utils import instantiate
from srcs.data_loader.data_loaders import get_data_loaders

# fix random seeds for reproducibility
SEED = 123
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(SEED)

logger = logging.getLogger('train')

@hydra.main(config_path='conf/', config_name='train')
def main(config):
    # setup data_loader instances
    print(config.data)
    data_loader, valid_data_loader = get_data_loaders(config.data)

    # build model. print it's structure and # trainable params.
    model = instantiate(config.arch, hparams=config.hparams)
    logger.info(model)
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    logger.info(f'Trainable parameters: {sum([p.numel() for p in trainable_params])}')

    # get function handles of loss and metrics
    criterion = instantiate(config.loss, is_func=True)
    metrics = [instantiate(met, is_func=True) for met in config['metrics']] if config['metrics'] else []
    
    # build optimizer, learning rate scheduler.
    optimizer = instantiate(config.optimizer, model.parameters())
    lr_scheduler = instantiate(config.lr_scheduler, optimizer)
    trainer = Trainer(model, criterion, metrics, optimizer,
                      config=config,
                      data_loader=data_loader,
                      valid_data_loader=valid_data_loader,
                      lr_scheduler=lr_scheduler)
    trainer.train()


if __name__ == '__main__':
    # pylint: disable=no-value-for-parameter
    main()
