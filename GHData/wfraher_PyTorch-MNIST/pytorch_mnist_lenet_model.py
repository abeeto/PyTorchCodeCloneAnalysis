# -*- coding: utf-8 -*-
"""pytorch MNIST LeNet model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jr5t31do3eIIC3w0ZFWa95khw0Vj1pge
"""

import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn 
import torch.nn.functional as F
import torch.optim as optim
import time

#Hyperparameters
epochs = 5
lr = 0.001
momentum = 0.9
stat_freq = 100 #how many mini-batches in between each point on the performance graph
width = 28
height = 28
num_channels = 1
batch_size = 16

#Loading the dataset

transform = transforms.Compose([
  transforms.Pad(2, padding_mode='edge'), #LeNet requires 32x32 inputs, this may need to be changed for other datasets.
  transforms.ToTensor()
])

trainset = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)

testset = torchvision.datasets.MNIST(root="./data", train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True, num_workers=2)

trainiter = iter(trainloader)
testiter = iter(testloader)

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()

#Defining the optimizer and loss function

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)

#Training the network

start_time = time.time()
loss_collection = []
episode_no = 0
for epoch in range(epochs):

  average_loss = 0.0

  for _, data in enumerate(trainloader):

    #data is a list of inputs and labels
    inputs, labels = data

    #clear the parameter gradients
    optimizer.zero_grad()

    #collect predictions
    outputs = net(inputs)

    #calculate loss and collect gradients
    loss = criterion(outputs,labels)
    loss.backward()

    #adjust weights
    optimizer.step()

    #add statistics to collection for graphing purposes
    average_loss += loss.item()

    episode_no += 1
    if _ % stat_freq == stat_freq - 1:
      print("Episode " + str(episode_no) + " finished with average loss " + str(average_loss / stat_freq))
      loss_collection.append(average_loss / stat_freq)
      average_loss = 0.0

print ("Elapsed time " + str(time.time() - start_time))

#Graph results when complete, show percent accuracy
plt.plot(loss_collection)
plt.show()

print("Final loss: " + str(loss_collection[len(loss_collection)-1]))

right = 0
wrong = 0
for inputs, labels in iter(testloader):

    with torch.no_grad():

      outputs = net(inputs)
      predictions = F.softmax(outputs,dim=1).argmax(axis=1)

      for p,l in zip(predictions, labels):
        if p == l:
          right += 1
        else:
          wrong += 1

print(str((right/(right + wrong)) * 100) + "% accuracy")

