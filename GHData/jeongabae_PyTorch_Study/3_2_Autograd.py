#경사 하강법 코드를 보고있으면 requires_grad=True, backward() 등이 나옵니다.
# 이는 파이토치에서 제공하고 있는 자동 미분(Autograd) 기능을 수행하고 있는 것

#1. 경사 하강법 리뷰
# 경사 하강법은 비용 함수를 미분하여 이 함수의 기울기(gradient)를 구해서 비용이 최소화 되는 방향을 찾아내는 알고리즘
#모델이 복잡해질수록 경사 하강법을 넘파이 등으로 직접 코딩하는 것은 까다로움.
# 파이토치에서는 이런 수고를 하지 않도록 자동 미분(Autograd) 지원

#2. 자동 미분(Autograd) 실습하기 : 임의로 2w^2+5라는 식을 세워보고, w에 대해 미분
import torch
"""
값이 2인 임의의 스칼라 텐서 w를 선언합니다. 이때 required_grad를 True로 설정합니다. 
이는 이 텐서에 대한 기울기를 저장하겠다는 의미입니다. 뒤에서 보겠지만, 이렇게 하면 w.grad에 w에 대한 미분값이 저장됩니다.
"""
w = torch.tensor(2.0, requires_grad=True)

#수식 정의
y = w**2
z = 2*y + 5

z.backward() # 해당 수식을 w에 대해서 미분해야합니다. .backward()를 호출하면 해당 수식의 w에 대한 기울기를 계산

#w.grad를 출력하면 w가 속한 수식을 w로 미분한 값이 저장된 것을 확인 가능
print('수식을 w로 미분한 값 : {}'.format(w.grad)) #수식을 w로 미분한 값 : 8.0
