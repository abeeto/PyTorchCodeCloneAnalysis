# -*- coding: utf-8 -*-
"""w3Coding.ipynb

Automatically generated by Colaboratory.

"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# Code tested and running in Google Colab using Python 3.6.7 and Torch 1.0.1.post2 (installed by default by Google)
# Code also tested and running locally (on a MacBook Pro) using Python 3.6.8 and Torch 1.0.0 (installed using conda)

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
log_interval = 300
learning_rate = 0.01
momentum = 0.01
epochs = 50

def get_dataset(batch_size = 64, test_batch_size = 64):
    train_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST('../data', train=True, download=True,
                       transform=transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.1307,), (0.3081,))
                       ])),
        batch_size=batch_size, shuffle=True)
    test_loader = torch.utils.data.DataLoader(
        datasets.FashionMNIST('../data', train=False, transform=transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.1307,), (0.3081,))
                       ])),
        batch_size=test_batch_size, shuffle=True)
    return train_loader, test_loader

class Net(nn.Module):
    def __init__(self, l1_dim, l2_dim, l3_dim):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, l1_dim)
        self.fc2 = nn.Linear(l1_dim, l2_dim)
        self.fc3 = nn.Linear(l2_dim, l3_dim)

    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.log_softmax(x, dim=1)

def train(model, device, train_loader, optimizer, epoch, log_interval):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
    return loss

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    occurrence = {}
    correct_label = {}
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            for i in range(len(pred)):
                if pred[i].data.cpu().numpy()[0] not in occurrence.keys():
                    occurrence[pred[i].data.cpu().numpy()[0]]=1
                    correct_label[pred[i].data.cpu().numpy()[0]]=0
                else:
                    occurrence[pred[i].data.cpu().numpy()[0]] = occurrence[pred[i].data.cpu().numpy()[0]]+1
                if target[i].data.cpu().numpy() == pred[i].data.cpu().numpy()[0]:
                    correct_label[pred[i].data.cpu().numpy()[0]]=correct_label[pred[i].data.cpu().numpy()[0]]+1
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    for key in occurrence:
        print("Class {}: Accuracy: {}/{} ({:.0f}%)".format(key, correct_label[key], occurrence[key], correct_label[key]/occurrence[key]*100))
    print()
    return test_loss

train_loader, test_loader = get_dataset()

model = Net(300, 100, 10).to(device)
optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
best_loss = 999
# For early stopping
stale_loss = 0
x_epoch = []
train_loss_ot = []
test_loss_ot = []

for epoch in range(1, epochs + 1):
    # training phase
    train_loss = train(model, device, train_loader, optimizer, epoch, log_interval)
    x_epoch.append(epoch)
    train_loss_ot.append(train_loss)
    
    # validation phase
    current_loss = test(model, device, test_loader)
    test_loss_ot.append(current_loss)
    if current_loss >= best_loss:
        stale_loss += 1
        if stale_loss >= 3:
            # Early stopping when loss does not decrease for 3 epochs
            break
    else:
        torch.save(model.state_dict(),"fashionmnist_fc.pt")
        best_loss = current_loss
        stale_loss = 0
        
print("Best Loss: {}".format(best_loss))

model = Net(300, 100, 10).to(device)
model.load_state_dict(torch.load("fashionmnist_fc.pt", map_location=device))
loss = test(model, device, test_loader)

plt.title("Train loss over epoch")
plt.xlabel('epoch')
plt.ylabel('loss')
plt.plot(x_epoch, train_loss_ot)
plt.show()

plt.title("Test loss over epoch")
plt.xlabel('epoch')
plt.ylabel('loss')
plt.plot(x_epoch, test_loss_ot)
plt.show()

