# -*- coding: utf-8 -*-
"""checkpointing_fuseconv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MY8SDrqYif0JWGC_JWEjW1oq5t76DR3J
"""

import math
import torch
import torchvision
import time
from matplotlib import pyplot as plt
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
import numpy as np
!pip install wandb
import wandb
!wandb login 4a30a34490a130dc21329fd04548bfd9f01cb1ec

batch_size = 200
learning_rate = 5e-3
momentum = 0.5
height = 224
width = 224
channels = 3
weight_decay = 1e-2
gamma = 0.75
momentum = 0.1
momentum = math.sqrt(0.1)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
gpu = torch.cuda.is_available()

class Hsigmoid(torch.nn.Module):
    def __init__(self, inplace = True):
        super(Hsigmoid, self).__init__() 
        self.inplace = inplace

    def forward(self, x):
        return torch.nn.functional.relu(x+3, inplace = self.inplace)/6.0

class SEModule(torch.nn.Module):
    def __init__(self, channel, reduction = 4):
        super(SEModule, self).__init__() 
        self.avg_pool = torch.nn.AdaptiveAvgPool2d(1)
        self.fc = torch.nn.Sequential(
            torch.nn.Linear(channel, channel//reduction, bias=False),
            torch.nn.ReLU(inplace=True),
            torch.nn.Linear(channel // reduction, channel, bias=False),
            Hsigmoid()
        )

    def _initialize_weights(self):
        # weight initialization
        for m in self.modules():
            if isinstance(m, torch.nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)
            elif isinstance(m, torch.nn.BatchNorm2d):
                torch.nn.init.ones_(m.weight)
                torch.nn.init.zeros_(m.bias)
            elif isinstance(m, torch.nn.Linear):
                torch.nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)

    def forward(self, x):
        b, c, _, _= x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x) 

class Hswish(torch.nn.Module):
    def __init__(self, inplace=True):
        super(Hswish, self).__init__()
        self.inplace = inplace
    
    def forward(self, x):
        return x * torch.nn.functional.relu6(x + 3., inplace=self.inplace) / 6.   

def get_training_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):
    """ return training dataloader
    Args:
        path: path to cifar100 training python dataset
        batch_size: dataloader batchsize
        num_workers: dataloader num_works
        shuffle: whether to shuffle
    Returns: train_data_loader:torch dataloader object
    """

    transform_train = torchvision.transforms.Compose([
        #torchvision.transforms.ToPILImage(),
        torchvision.transforms.RandomCrop(32, padding=4),
        torchvision.transforms.RandomHorizontalFlip(),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean, std)
    ])
    #cifar100_training = CIFAR100Train(path, transform=transform_train)
    cifar100_training = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
    cifar100_training_loader = torch.utils.data.DataLoader(
        cifar100_training, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

    return cifar100_training_loader

def get_test_dataloader(mean, std, batch_size=16, num_workers=2, shuffle=True):
    """ return training dataloader
    Args:
        path: path to cifar100 test python dataset
        batch_size: dataloader batchsize
        num_workers: dataloader num_works
        shuffle: whether to shuffle
    Returns: cifar100_test_loader:torch dataloader object
    """

    transform_test = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean, std)
    ])
    #cifar100_test = CIFAR100Test(path, transform=transform_test)
    cifar100_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)
    cifar100_test_loader = torch.utils.data.DataLoader(
        cifar100_test, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size)

    return cifar100_test_loader

class FuseBlock(torch.nn.Module):
    def __init__(self, K, C, stride, is_SE, NL, exp, oup):
        super(FuseBlock, self).__init__()
        self.K = K
        self.C = C
        self.stride = stride, 
        self.is_SE = is_SE,
        self.NL = NL
        self.exp = exp
        self.oup = oup
        self.conv1 = torch.nn.Conv2d(
            in_channels = self.C,
            out_channels = self.exp,                   
            kernel_size = 1,
            stride = 1,
            padding = 0
        )
        self.bn1 = torch.nn.BatchNorm2d(
            num_features = self.exp,
            momentum = momentum
        )
        self.conv2 = torch.nn.Conv2d(
            in_channels = self.exp,
            out_channels = self.exp,
            kernel_size = (1, self.K),
            stride = self.stride,
            padding = (0,int((self.K-1)/2)),
            groups = self.exp,
        )
        self.bn2 = torch.nn.BatchNorm2d(
            num_features = self.exp,
            momentum = momentum
        )
        self.conv3 = torch.nn.Conv2d(
            in_channels = self.exp,
            out_channels = self.exp,
            kernel_size = (self.K, 1),
            stride = self.stride,
            padding = (int((self.K-1)/2),0),
            groups = self.exp,
        )
        self.bn3 = torch.nn.BatchNorm2d(
            num_features = self.exp,
            momentum = momentum
        )
        self.hsigmoid = Hsigmoid()
        self.SE = SEModule(channel = 2*self.exp)
        self.conv4 = torch.nn.Conv2d(
            in_channels = 2*self.exp,
            out_channels = self.oup,
            stride = 1,
            kernel_size = 1,
            padding = 0
        )
        self.bn4 = torch.nn.BatchNorm2d(
            num_features = self.oup,
            momentum = momentum
        )

    def _initialize_weights(self):
        # weight initialization
        for m in self.modules():
            if isinstance(m, torch.nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)
            elif isinstance(m, torch.nn.BatchNorm2d):
                torch.nn.init.ones_(m.weight)
                torch.nn.init.zeros_(m.bias)
            elif isinstance(m, torch.nn.Linear):
                torch.nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias) 

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.NL(x)
        #print('x')
        #print(x.size())
        x1 = self.conv2(x)
        x2 = self.conv3(x)
        x1 = self.bn2(x1)   
        x2 = self.bn3(x2)
        #print('x1')
        #print(x1.size())
        #print('x2')
        #print(x2.size())
        x = torch.cat([x1, x2], 1)
        if self.is_SE:
            x = self.SE(x)
            x = self.hsigmoid(x)
        x = self.NL(x)
        x = self.conv4(x)
        x = self.bn4(x)
        return x

class FuseNet(torch.nn.Module):
    def __init__(self, H, W, C):
        super(FuseNet, self).__init__()
        self.relu = torch.nn.ReLU()
        self.hswish = Hswish()
        self.conv1 = torch.nn.Conv2d(
            in_channels = C,
            out_channels = 16,
            stride = 2, 
            kernel_size = 3, 
            padding = 1 ,
        )
        self.fuse1 = FuseBlock(
            K = 3,
            C = 16,
            stride = 2,
            is_SE = True, 
            NL = self.relu,
            exp = 16, 
            oup = 16
        )

        self.fuse2 = FuseBlock(
            K = 3,
            C = 16,
            stride = 2,
            is_SE = False, 
            NL = self.relu,
            exp = 72, 
            oup = 24
        ) 

        self.fuse3 = FuseBlock(
            K = 3,
            C = 24,
            stride = 1,
            is_SE = False, 
            NL = self.relu,
            exp = 88, 
            oup = 24
        ) 
        
        self.fuse4 = FuseBlock(
            K = 5,
            C = 24,
            stride = 2,
            is_SE = True, 
            NL = self.hswish,
            exp = 96, 
            oup = 40
        ) 

        self.fuse5 = FuseBlock(
            K = 5,
            C = 40,
            stride = 1,
            is_SE = True, 
            NL = self.hswish,
            exp = 240,
            oup = 40
        )

        self.fuse6 = FuseBlock(
            K = 5,
            C = 40,
            stride = 1,
            is_SE = True,
            NL = self.hswish,
            exp = 240,
            oup = 40
        )
        
        self.fuse7 = FuseBlock(
            K = 5,
            C = 40,
            stride = 1,
            is_SE = True,
            NL = self.hswish,
            exp = 120,
            oup = 48
        )
        
        self.fuse8 = FuseBlock(
            K = 5, 
            C = 48, 
            stride = 1, 
            is_SE = True, 
            NL = self.hswish,
            exp = 144, 
            oup = 48
        )

        self.fuse9 = FuseBlock(
            K = 5, 
            C = 48, 
            stride = 2, 
            is_SE = True, 
            NL = self.hswish,
            exp = 288,
            oup = 96
        )

        self.fuse10 = FuseBlock(
            K = 5,
            C = 96, 
            stride = 1, 
            is_SE = True, 
            NL = self.hswish,
            exp = 576, 
            oup = 96, 
        )

        self.fuse11 = FuseBlock(
            K = 5, 
            C = 96, 
            stride = 1, 
            is_SE = True, 
            NL = self.hswish, 
            exp = 576, 
            oup = 96, 
        )

        self.conv2 = torch.nn.Conv2d(
            in_channels = 96,
            out_channels = 576,
            stride = 1,
            kernel_size = 1,
            padding = 0,
        )

        self.pool = torch.nn.AdaptiveAvgPool2d(
            output_size = (1, 1)
        )
        
        self.conv3 = torch.nn.Conv2d(
            in_channels = 576,
            out_channels = 1024,
            stride = 1,
            kernel_size = 1,
            padding = 0,
        )

        self.dropout = torch.nn.Dropout2d(p=0.2)    

        self.conv4 = torch.nn.Conv2d(
            in_channels = 1024,
            out_channels = 100,
            stride = 1,
            kernel_size = 1,
            padding = 0,
        )
        self.exp = self.baseline
        self.layers = [
            self.conv1,
            self.hswish,
            self.fuse1,
            self.fuse2,
            self.fuse3,
            self.fuse4,
            self.fuse5,
            self.fuse6,
            self.fuse7,
            self.fuse8,
            self.fuse9,
            self.fuse10,
            self.fuse11,
            self.conv2,
            self.pool,
            self.conv3
        ]
        self.i = 0
        self.j = len(self.layers)-1

    def set_exp(self, exp):
        if exp == 'baseline':
            self.exp = self.baseline
        elif exp == 'exp1':
            self.exp = self.exp1
        elif exp == 'exp2':
            self.exp = self.exp2
        elif exp == 'exp3':
            self.exp = self.exp3
        elif exp == 'exp4':
            self.exp = self.exp4
        elif exp == 'exp5':
            self.exp = self.exp5

    def _initialize_weights(self):
        # weight initialization
        for m in self.modules():
            if isinstance(m, torch.nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)
            elif isinstance(m, torch.nn.BatchNorm2d):
                torch.nn.init.ones_(m.weight)
                torch.nn.init.zeros_(m.bias)
            elif isinstance(m, torch.nn.Linear):
                torch.nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    torch.nn.init.zeros_(m.bias)
    
    def get_mem_usage(self):
        return torch.cuda.memory_allocated()

    def set_ij(self, i, j):
        self.i = i
        self.j = j
        self.exp = self.method

    def method(self, x):
        """
            The Dropout layer must be checkpointed
            The remaining checkpoints must be one of the 11 Fuse Blocks
            Running a grid search can help find the most appropriate output
        """
        for i, l in enumerate(self.layers):
            if i == self.i or i == self.j:
                x = torch.utils.checkpoint.checkpoint(l, x)
            else:
                x = l(x)
        x = torch.utils.checkpoint.checkpoint(self.dropout, x)  
        x = self.conv4(x)
        b = x.size()[0]
        x = torch.reshape(x, (b, 100))
        return x

    def baseline(self, x):
        x = self.conv1(x)
        x = self.hswish(x)
        x = self.fuse1(x)
        x = self.fuse2(x)
        x = self.fuse3(x)
        x = self.fuse4(x)
        x = self.fuse5(x)
        x = self.fuse6(x)
        x = self.fuse7(x)
        x = self.fuse8(x)
        x = self.fuse9(x)
        x = self.fuse10(x)
        x = self.fuse11(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = self.dropout(x)
        x = self.conv4(x)
        b = x.size()[0]
        x = torch.reshape(x, (b, 100))
        return x

    def exp1(self, x):
        x = self.conv1(x)
        x = self.hswish(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse1, x)
        x = self.fuse2(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse3, x)
        x = self.fuse4(x)
        x = self.fuse5(x)
        x = self.fuse6(x)
        x = self.fuse7(x)
        x = self.fuse8(x)
        x = self.fuse9(x)
        x = self.fuse10(x)
        x = self.fuse11(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = torch.utils.checkpoint.checkpoint(self.dropout, x)  
        x = self.conv4(x)
        b = x.size()[0]
        x = torch.reshape(x, (b, 100))
        return x

    def exp2(self, x):
        """
            The peak in the baseline is at fuse 10, so the first checkpoint must be there
            The remaining fuse blocks will be distributed around the checkpoint layer uniformly
        """
        x = torch.utils.checkpoint.checkpoint(self.conv1, x)
        x = self.hswish(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse1, x)
        x = self.fuse2(x)
        x = self.fuse3(x)
        x = self.fuse4(x)
        x = self.fuse5(x)
        x = self.fuse6(x)
        x = self.fuse7(x)
        x = self.fuse8(x)
        x = self.fuse9(x)
        x = self.fuse10(x)
        x = self.fuse11(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = torch.utils.checkpoint.checkpoint(self.dropout, x)  
        x = self.conv4(x)
        b = x.size()[0]
        x = torch.reshape(x, (b, 100))
        return x

    def exp3(self, x):
        x = self.conv1(x)
        x = torch.utils.checkpoint.checkpoint(self.hswish, x)
        x = self.fuse1(x)
        x = self.fuse2(x)
        x = self.fuse3(x)
        x = self.fuse4(x)
        x = self.fuse5(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse6, x)
        x = self.fuse7(x)
        x = self.fuse8(x)
        x = self.fuse9(x)
        x = self.fuse10(x)
        x = self.fuse11(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = torch.utils.checkpoint.checkpoint(self.dropout, x)  
        x = self.conv4(x)
        b = x.size()[0]
        x = torch.reshape(x, (b, 100))
        return x
    
    def exp4(self, x):
        x = self.conv1(x)
        x = self.hswish(x)
        x = self.fuse1(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse2, x)
        x = self.fuse3(x)
        x = self.fuse4(x)
        x = self.fuse5(x)
        x = self.fuse6(x)
        x = self.fuse7(x)
        x = self.fuse8(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse9, x)
        x = self.fuse10(x)
        x = self.fuse11(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = torch.utils.checkpoint.checkpoint(self.dropout, x)  
        x = self.conv4(x)
        b = x.size()[0]
        x = torch.reshape(x, (b, 100))
        return x

    def exp5(self, x):
        x = self.conv1(x)
        x = self.hswish(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse1, x)
        x = self.fuse2(x)
        x = self.fuse3(x)
        x = self.fuse4(x)
        x = self.fuse5(x)
        x = self.fuse6(x)
        x = self.fuse7(x)
        x = torch.utils.checkpoint.checkpoint(self.fuse8, x)
        x = self.fuse9(x)
        x = self.fuse10(x)
        x = self.fuse11(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = torch.utils.checkpoint.checkpoint(self.dropout, x)  
        x = self.conv4(x)
        b = x.size()[0]
        x = torch.reshape(x, (b, 100))
        return x

    def forward(self, x):
        return self.exp(x)

from conf import settings
loss_function = torch.nn.CrossEntropyLoss()

cifar100_training_loader = get_training_dataloader(
    settings.CIFAR100_TRAIN_MEAN,
    settings.CIFAR100_TRAIN_STD,
    num_workers=4,
    batch_size=batch_size,
    shuffle=True
)

cifar100_test_loader = get_test_dataloader(
    settings.CIFAR100_TRAIN_MEAN,
    settings.CIFAR100_TRAIN_STD,
    num_workers=4,
    batch_size=batch_size,
    shuffle=True
)

def _get_gpu_mem(synchronize=True, empty_cache=True):
    return torch.cuda.memory_allocated(), torch.cuda.memory_cached()


def _generate_mem_hook(handle_ref, mem, idx, hook_type, exp):
    def hook(self, *args):
        if len(mem) == 0 or mem[-1]["exp"] != exp:
            call_idx = 0
        else:
            call_idx = mem[-1]["call_idx"] + 1

        mem_all, mem_cached = _get_gpu_mem()
        torch.cuda.synchronize()
        mem.append({
            'layer_idx': idx,
            'call_idx': call_idx,
            'layer_type': type(self).__name__,
            'exp': exp,
            'hook_type': hook_type,
            'mem_all': mem_all,
            'mem_cached': mem_cached,
        })

    return hook


def _add_memory_hooks(idx, mod, mem_log, exp, hr):
    h = mod.register_forward_pre_hook(_generate_mem_hook(hr, mem_log, idx, 'pre', exp))
    hr.append(h)

    h = mod.register_forward_hook(_generate_mem_hook(hr, mem_log, idx, 'fwd', exp))
    hr.append(h)

    h = mod.register_backward_hook(_generate_mem_hook(hr, mem_log, idx, 'bwd', exp))
    hr.append(h)

def log_mem(model, inp, mem_log=None, exp=None):
    mem_log = mem_log or []
    exp = exp or f'exp_{len(mem_log)}'
    hr = []
    for idx, module in enumerate(model.modules()):
        _add_memory_hooks(idx, module, mem_log, exp, hr)
    out = model(inp)
    loss = out.sum()
    loss.backward()
    [h.remove() for h in hr]
    return mem_log

def pp(df, exp):
    df_exp = df[df.exp == exp]
    df_pprint = (
        df_exp.assign(
            open_layer=lambda ddf: ddf.hook_type.map(
                lambda x: {"pre": 0, "fwd": 1, "bwd": 2}[x]).rolling(2).apply(lambda x: x[0] == 0 and x[1] == 0
                                                                              )
        )
            .assign(
            close_layer=lambda ddf: ddf.hook_type.map(
                lambda x: {"pre": 0, "fwd": 1, "bwd": 2}[x]).rolling(2).apply(lambda x: x[0] == 1 and x[1] == 1)
        )
            .assign(indent_level=lambda ddf: (ddf.open_layer.cumsum() - ddf.close_layer.cumsum()).fillna(0).map(int))
            .sort_values(by="call_idx")
            .assign(mem_diff=lambda ddf: ddf.mem_all.diff() // 2 ** 20)
    )
    pprint_lines = [
        f"{'    ' * row[1].indent_level}{row[1].layer_type} {row[1].hook_type}  {row[1].mem_diff or ''}"
        for row in df_pprint.iterrows()
    ]
    for x in pprint_lines:
        print(x)


def plot_mem(
        df,
        exps=None,
        normalize_call_idx=False,
        normalize_mem_all=True,
        filter_fwd=False,
        return_df=False,
        output_file=None
):
    if exps is None:
        exps = df.exp.drop_duplicates()

    fig, ax = plt.subplots(1, 1, figsize=(20, 10))
    for exp in exps:
        df_ = df[df.exp == exp]

        if normalize_call_idx:
            df_.call_idx = df_.call_idx / df_.call_idx.max()

        if normalize_mem_all:
            df_.mem_all = df_.mem_all - df_[df_.call_idx == df_.call_idx.min()].mem_all.iloc[0]
            df_.mem_all = df_.mem_all // 2 ** 20

        if filter_fwd:
            layer_idx = 0
            callidx_stop = df_[(df_["layer_idx"] == layer_idx) & (df_["hook_type"] == "fwd")]["call_idx"].iloc[0]
            df_ = df_[df_["call_idx"] <= callidx_stop]
            # df_ = df_[df_.call_idx < df_[df_.layer_idx=='bwd'].call_idx.min()]
        x = df_['call_idx'].values.tolist()
        y = df_['mem_all'].values.tolist()
        ax.plot(x, y, label = exp)
        #plot = df_.plot(ax=ax, x='call_idx', y='mem_all', label=exp)
        if output_file:
            #plot.get_figure().savefig(output_file)
            fig.savefig(output_file)

    if return_df:
        return df_

"""
# GRID SEARCH
from tqdm import tqdm
net = FuseNet(height, width, channels)
state_dict = torch.load('weights/fuse-170-best.pth')
net.load_state_dict(state_dict)
net = net.cuda()
res = {}
plots = []
fig, ax = plt.subplots(1, 1, figsize=(40, 20))
max_mem = 1e30
mem_log = []
st = 2
en = len(net.layers) - 1
for i in tqdm(range(st, en)):
    for j in range(st, en):
        if i == j or i+1 == j or j+1 == i: 
            continue
        else:
            net.set_ij(i, j)
            exp = 'fusenet_{i}_{j}'.format(i=i, j=j)
            batch_index, (images, labels)  = next(enumerate(cifar100_training_loader))
            if gpu:
                images = images.cuda()
                labels = labels.cuda()
            start = time.time()
            mem = log_mem(net, images, mem_log, exp)
            end = time.time()
            df = pd.DataFrame(mem)
            res[exp] = df
            y = df['mem_all'].values.tolist()
            x = df['call_idx'].values.tolist()
            plots.append({'x':x,'y':y,'i':i,'j':j,'exp':exp})

val = None
for plot in plots:
    ax.plot(plot['x'], plot['y'], label = plot['exp'])
    if max(y)<max_mem:
        val = (plot['i'], plot['j'])
        max_mem = max(y)
ax.legend()
fig.savefig('fusenet.png')
plt.show()
plt.close('all')
"""

exp_list = ['baseline',  'exp1', 'exp2', 'exp3', 'exp4', 'exp5']
mem_log = []
net = FuseNet(height, width, channels)
state_dict = torch.load('weights/fuse-170-best.pth')
net.load_state_dict(state_dict)
net = net.cuda()
fig, ax = plt.subplots(1, 1, figsize=(20, 10))
res = {}
val = []
for exp in exp_list:
    wandb.init(project="assignment6", entity="shandilya1998", name = 'fuseconv_{ex}'.format(ex = exp), id = 'fuseconv_{ex}'.format(ex = exp))
    net.set_exp(exp)
    time_lst = []
    batch_index, (images, labels)  = next(enumerate(cifar100_training_loader))
    if gpu:
        images = images.cuda()
        labels = labels.cuda()
    start = time.time()
    mem = log_mem(net, images, mem_log, exp)
    end = time.time()
    df = pd.DataFrame(mem)
    res[exp] = df
    df_ = df[df.exp == exp]
    df_.call_idx = df_.call_idx / df_.call_idx.max()
    df_.mem_all = df_.mem_all - df_[df_.call_idx == df_.call_idx.min()].mem_all.iloc[0]
    df_.mem_all = df_.mem_all // 2 ** 20
    x = df_['call_idx'].values.tolist()
    y = df_['mem_all'].values.tolist()
    ax.plot(x, y, label = exp)
    [wandb.log(me) for me in mem]
    wandb.config['run time'] = end - start
ax.legend()
fig.savefig('fusenet.png')
plt.show()
plt.close('all')

1e10

import numpy as np
np.array([1,2,4]).max()

val

max([8,9,0])

