
# Solve \Delta u + k*u = f by NN
# Author: Zhiwei Fang
# Email: fangz1@unlv.nevada.edu
# Copyright reserved


import torch
import numpy as np
from pyDOE import lhs
import time
from scipy.interpolate import griddata
import matplotlib.pyplot as plt

USE_GPU = torch.cuda.is_available()
DO_DROPOUT = False
SHOW_STEPS = 10  # show the loss function during training every SHOW_STEPS steps
LBFGS_MAX_ITER = 50000  # the maximal iteration steps for LBFGS
torch.set_default_dtype(torch.float64)  # double precision


np.random.seed(1234)       # reproducible
torch.manual_seed(1234)    # reproducible


class Ellptic():
    # constructor:
    # p_bnd: sample points on boundary, [x_bnd, y_bnd]
    # u_bnd: boundary function data, same row size with p_bnd!
    # p_int: sample points of interior, [x_int, y_int]
    # rhs_in: rhs function data, same row size with p_int
    # layers: number of neuron for each layer
    # dprs: dropout rates, same size with layers, but the 1st and last ones will not use!
    # px, py: grid points of partition
    # k: the coefficient in the PDE
    # lub=[lb, ub], lb = [xa, ya], ub = [xb, yb], are the bounds for the computational domain, numpy arraies
    # data format:
    # [x0,y0; x1,y1; x2,y2; ...;xn,yn]
    def __init__(self, **kwargs):
        # unzip the parameters
        self.p_bnd = kwargs['p_bnd']
        self.u_bnd = kwargs['u_bnd']
        self.p_in = kwargs['p_in']
        self.rhs_in = kwargs['rhs_in']
        self.layers = kwargs['layers']
        self.dprs = kwargs['dprs']
        self.k = kwargs['k']
        self.lb, self.ub = kwargs['lub']
        # define PINN
        self.pinn = self.PINN(**kwargs)
        if USE_GPU: self.pinn.cuda()
        # define the optimizer
        self.optimizer1 = torch.optim.Adam(self.pinn.parameters())
        self.optimizer2 = torch.optim.LBFGS(self.pinn.parameters(), max_iter=LBFGS_MAX_ITER)
        # define the loss function
        self.loss_func = torch.nn.MSELoss()
        if USE_GPU: self.loss_func.cuda()

    def net_u(self, x, y):  # predict the solution
        XY = torch.cat((x, y), 1)
        u = self.pinn(XY)
        return u

    def net_u_f(self, x, y):  # define the residue of the PDE
        u = self.net_u(x, y)
        x_shape = torch.ones(x.size())
        y_shape = torch.ones(y.size())
        if USE_GPU: x_shape, y_shape = x_shape.cuda(), y_shape.cuda()
        # first compute inner product: outputs*grad_outputs=scalar, then compute scalar/x and scalar/y
        # du = [u_x, u_y]
        du = torch.autograd.grad([u,0.*u], [x,y], grad_outputs=[x_shape, y_shape], create_graph=True)
        u_xx = torch.autograd.grad(du[0].sum(), x, create_graph=True)[0]
        u_yy = torch.autograd.grad(du[0].sum(), y, create_graph=True)[0]

        # u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]
        # u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]
        # u_y = torch.autograd.grad(u.sum(), y, create_graph=True)[0]
        # u_yy = torch.autograd.grad(u_y.sum(), y, create_graph=True)[0]

        f = u_xx + u_yy + self.k * u
        return f

    def closure(self):
        self.optimizer2.zero_grad()
        ubnd_pred = self.net_u(self.p_bnd[:, 0:1], self.p_bnd[:, 1:2])
        f_pred = self.net_u_f(self.p_in[:, 0:1], self.p_in[:, 1:2])
        loss1 = self.loss_func(ubnd_pred, self.u_bnd)
        loss2 = self.loss_func(f_pred, self.rhs_in)
        loss = loss1 + loss2
        print('Loss=%.3e' % loss.item())
        loss.backward()
        return loss

    def train_net(self, epoch):
        self.pinn.train()
        print('Adam optimization')
        start_time = time.time()
        for i in range(epoch):
            ubnd_pred = self.net_u(self.p_bnd[:, 0:1], self.p_bnd[:, 1:2])
            f_pred = self.net_u_f(self.p_in[:, 0:1], self.p_in[:, 1:2])
            loss1 = self.loss_func(ubnd_pred, self.u_bnd)
            loss2 = self.loss_func(f_pred, self.rhs_in)
            loss = loss1 + loss2
            # Adam optimization
            self.optimizer1.zero_grad()
            loss.backward(retain_graph=True)
            self.optimizer1.step()
            if i % SHOW_STEPS == 0:
                elapsed = time.time() - start_time
                print('Epoch: %d / %d, Loss=%.3e, Time: %.2f' % (i, epoch, loss.item(), elapsed))
                start_time = time.time()
        print('LBFGS optimization')
        self.optimizer2.step(self.closure)

    def post(self, u, x):
    # u is exact solution on x, x=[x_data,y_data],
    # x_data and y_data are generated by meshgrid.
    # All inputs are numpy arries!
        # predict the solution at x
        xy_g = np.hstack((x[0].flatten()[:, None], x[1].flatten()[:, None]))
        xy_tensor = torch.from_numpy(xy_g).cuda() if USE_GPU else torch.from_numpy(xy_g)

        self.pinn.eval()
        u_h = self.net_u(xy_tensor[:, 0:1], xy_tensor[:, 1:2])
        # compute the relative error
        u_h_data = u_h.data.cpu().numpy() if USE_GPU else u_h.data.numpy()
        u_data = u(xy_g[:, 0:1], xy_g[:, 1:2])
        error_u = np.linalg.norm(u_h_data - u_data, 2) / np.linalg.norm(u_data, 2)
        print('Relative Error (Cross Validation): %.3e' % error_u)
        # plot the numerical and exact solution
        uh_plot = griddata(xy_g, u_h_data.flatten(), (x[0], x[1]), method='cubic')
        fig = plt.figure()
        ax = fig.add_subplot(211)
        h1 = ax.imshow(uh_plot.T, interpolation='nearest', cmap='jet',
                      extent=[self.lb[1], self.ub[1], self.lb[0], self.ub[0]],
                      origin='lower', aspect=1.)
        ax.set_title('output of PINN')
        fig.colorbar(h1)

        u_plot = griddata(xy_g, u_data.flatten(), (x[0], x[1]), method='cubic')
        ax = fig.add_subplot(212)
        h2 = ax.imshow(u_plot.T, interpolation='nearest', cmap='jet',
                  extent=[self.lb[1], self.ub[1], self.lb[0], self.ub[0]],
                  origin='lower', aspect=1.)
        fig.colorbar(h2)
        ax.set_title('Exact Solution')
        plt.show()

    class PINN(torch.nn.Module):
        def __init__(self, **kwargs):
            # super(self.__class__, self).__init__()
            # super(Ellptic.PINN, self).__init__()
            torch.nn.Module.__init__(self)
            self.layers = kwargs['layers']
            self.dprs = kwargs['dprs']
            self.lb, self.ub = kwargs['lub']
            # transfer lb and ub to tensor for normalization
            self.lb = torch.unsqueeze(torch.from_numpy(self.lb), dim=0)
            self.ub = torch.unsqueeze(torch.from_numpy(self.ub), dim=0)
            if USE_GPU: self.lb, self.ub = self.lb.cuda(), self.ub.cuda()
            # define the layers
            self.fcs = []
            self.dps = []
            num_layers = len(self.layers)
            for i in range(num_layers - 2):  # define the hidden layers
                # linear layer
                fc = torch.nn.Linear(self.layers[i], self.layers[i + 1])
                setattr(self, 'fc%i' % i, fc)  # IMPORTANT set layer to the Module
                self.initialize_NN(fc)
                self.fcs.append(fc)
                # dropout
                if DO_DROPOUT:
                    dp = torch.nn.Dropout(p=self.dprs[i])
                    setattr(self, 'dp%i' % i, dp)  # IMPORTANT set layer to the Module
                    self.dps.append(dp)
            # define the output layer:
            fc = torch.nn.Linear(self.layers[-2], self.layers[-1])
            setattr(self, 'fc%i' % (num_layers - 2), fc)  # IMPORTANT set layer to the Module
            self.fcs.append(fc)

        def forward(self, x):
            # x->norm->(linear->dropout->act)->linear->...->linear->y
            num_layers = len(self.layers)
            x = 2. * (x - self.lb) / (self.ub - self.lb) - 1.   # normalization
            for i in range(num_layers - 2):  # hidden layers' map
                x = self.fcs[i](x)  # linear
                if DO_DROPOUT: x = self.dps[i](x)  # dropout
                x = torch.sin(np.pi * x)  # activation
            y = self.fcs[-1](x)
            return y


        def initialize_NN(self, layer):
            torch.nn.init.xavier_normal_(layer.weight)
            torch.nn.init.constant_(layer.bias, 0.0)


if __name__ == "__main__":
    # Define the domain: [xa,xb]*[ya,yb]
    xa, xb, ya, yb = 0, 1, 0, 1
    k = 0.0  # frequency k
    Np = 100  # partition points for each boundary
    Nb = 50 * 4  # sample number of every boundary
    Nf = 20000  # sample number in the interior of the domain
    # number of neuron for each layer, must greater than 4 elements
    layers = [2, 100, 100, 100, 100, 1]
    # dropout rate for each layer, same size to layers, but the 1st and last ones will not use:
    dprs = np.ones((1, len(layers))) * 0.0
    dprs = dprs.tolist()[0]
    # define exact solution, rhs and boundary value
    u_exa = lambda x, y: np.sin(x * np.exp(y))
    rhs = lambda x, y: (k - x**2*np.exp(2.*y) - np.exp(2.*y))*np.sin(x*np.exp(y)) + x*np.exp(y)*np.cos(x*np.exp(y))
    # partition points
    # at the last of the program, use all these points to do the prediction
    px = np.linspace(xa, xb, Np).reshape(-1, 1)
    py = np.linspace(ya, yb, Np).reshape(-1, 1)
    X, Y = np.meshgrid(px, py)  # grid points
    # generate boundary points
    p_bnd1 = np.concatenate((px, 0.0 * py + py[0]), 1)  # (x,ya)
    p_bnd2 = np.concatenate((0.0 * px + px[-1], py), 1)  # (xb,y)
    p_bnd3 = np.concatenate((px, 0.0 * py + py[-1]), 1)  # (x,yb)
    p_bnd4 = np.concatenate((0.0 * px + px[0], py), 1)  # (xa,y)
    p_bnd = np.concatenate((p_bnd1, p_bnd2, p_bnd3, p_bnd4), 0)  # union all the boundaries
    p_bnd = np.unique(p_bnd, axis=0)  # delete the repeated elements (corner points)
    u_bnd = u_exa(p_bnd[:, 0:1], p_bnd[:, 1:2])  # the corresponding function value on the boundary
    # pick the boundary points for training
    id_b = np.random.choice(p_bnd.shape[0], Nb, replace=False)
    p_bnd_tr = p_bnd[id_b, :]  # boundary points for training
    u_bnd_tr = u_bnd[id_b, :]  # function values for training
    # pick the interior pionts for training
    lb = np.array([xa, ya])
    ub = np.array([xb, yb])
    # interior training points. These points may not on the grid
    p_in = lb + (ub - lb) * lhs(2, Nf)      # XY_f
    rhs_in = rhs(p_in[:, 0:1], p_in[:, 1:2])
    # convert everything to tensor and CUDA is applicable
    convert_list = [p_bnd, u_bnd, p_bnd_tr, u_bnd_tr, p_in, rhs_in]
    if (USE_GPU):
        [p_bnd, u_bnd, p_bnd_tr, u_bnd_tr, p_in, rhs_in] = [torch.from_numpy(l).cuda() for l in
                                                                                convert_list]
    else:
        [p_bnd, u_bnd, p_bnd_tr, u_bnd_tr, p_in, rhs_in] = [torch.from_numpy(l) for l in
                                                                                convert_list]
    convert_list = [p_bnd, u_bnd, p_bnd_tr, u_bnd_tr, p_in, rhs_in]
    for l in convert_list:
        l.requires_grad = True
    kwargs = {'p_bnd': p_bnd, 'u_bnd': u_bnd, 'p_in': p_in, 'rhs_in': rhs_in, 'layers': layers, 'dprs': dprs,
              'k': k, 'lub': [lb, ub]}
    net = Ellptic(**kwargs)
    net.train_net(1000)
    net.post(u_exa, [X, Y])
