{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse GHData_functions-blind-clones/GHData_functions-blind-clones-0.30-classes-withsource.xml\n",
    "# to get the all <source> tags in each <class> tag\n",
    "# and write them to a file\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "xml_file = \"../../GHData_functions-blind-clones/GHData_functions-blind-clones-0.30-classes-withsource.xml\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def remove_non_alpha_characters_inside_source_tags(xml_content):\n",
    "    \"\"\"\n",
    "    Remove non-alphabetic characters (except spaces) inside <source> tags.\n",
    "    \"\"\"\n",
    "    # Regular expression to find content within <source> tags\n",
    "    source_tag_pattern = re.compile(r'(<source[^>]*>)(.*?)(</source>)', re.DOTALL)\n",
    "    \n",
    "    def remove_non_alpha_except_space(match):\n",
    "        # Keep only alphabetic characters and spaces within the content of the <source> tag\n",
    "        cleaned_content = re.sub(r'[^a-zA-Z\\s]', ' ', match.group(2))\n",
    "        # Reconstruct the <source> tag with cleaned content\n",
    "        return match.group(1) + cleaned_content + match.group(3)\n",
    "    \n",
    "    # Apply the cleaning function to all occurrences of <source> tags\n",
    "    return source_tag_pattern.sub(remove_non_alpha_except_space, xml_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse xml file\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# make fileToClean get the file content \n",
    "with open(xml_file, 'r') as file:\n",
    "    fileToClean = file.read()\n",
    "cleanedContent = remove_non_alpha_characters_inside_source_tags(fileToClean)\n",
    "\n",
    "# over write cleaned content \n",
    "with open(\"../../analysis/RQ3/cleaned.xml\", 'w') as file:\n",
    "    file.write(cleanedContent)\n",
    "\n",
    "tree = ET.parse(\"../../analysis/RQ3/cleaned.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# get elements by tag name\n",
    "extractedCode = []\n",
    "for class_ in root.findall(\"class\"):\n",
    "    appendedFile = \"\"\n",
    "    for source in class_.findall(\"source\"):\n",
    "        appendedFile += source.text\n",
    "    extractedCode.append(appendedFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import math as Math\n",
    "allBucketsInOne = ''\n",
    "for class_ in root.findall(\"class\"):\n",
    "    appendedFile = \"\"\n",
    "    sources_for_class = class_.findall(\"source\")\n",
    "    random_source = sources_for_class[ Math.floor( random() * len(sources_for_class) ) ]\n",
    "    appendedFile += random_source.text\n",
    "    allBucketsInOne += appendedFile\n",
    "with open(\"../../analysis/RQ3/allBucketsInOne.txt\", 'w') as file:\n",
    "    file.write(allBucketsInOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/abeeto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/abeeto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/abeeto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def update self  memory  \\n          Monte Carlo estimate of rewards \\n        rewards     \\n        discounted reward    \\n        for reward  is terminal in zip reversed memory rewards   reversed memory is terminals   \\n            if is terminal \\n                discounted reward    \\n            discounted reward   reward    self gamma   discounted reward \\n            rewards insert    discounted reward \\n        \\n          Normalizing the rewards \\n        rewards   torch tensor rewards  to device \\n        rewards    rewards   rewards mean     rewards std    e   \\n        \\n          convert list to tensor\\n        old states   torch squeeze torch stack memory states  to device      detach  \\n        old actions   torch squeeze torch stack memory actions  to device      detach  \\n        old logprobs   torch squeeze torch stack memory logprobs      to device  detach  \\n        \\n          Optimize policy for K epochs \\n        for   in range self K epochs  \\n              Evaluating old actions and values  \\n            logprobs  state values  dist entropy   self policy evaluate old states  old actions \\n            \\n              Finding the ratio  pi theta   pi theta  old  \\n            ratios   torch exp logprobs   old logprobs detach   \\n\\n              Finding Surrogate Loss \\n            advantages   rewards   state values detach     \\n            surr    ratios   advantages\\n            surr    torch clamp ratios    self eps clip    self eps clip  advantages\\n            loss    torch min surr   surr       self MseLoss state values  rewards       dist entropy\\n            \\n              take gradient step\\n            self optimizer zero grad  \\n            loss mean   backward  \\n            self optimizer step  \\n            \\n          Copy new weights into old policy \\n        self policy old load state dict self policy state dict   \\n        \\n\\n    def update self  memory  \\n          Monte Carlo estimation of rewards\\n        rewards     \\n        discounted reward    \\n        for reward  is terminal in zip reversed memory rewards   reversed memory is terminals   \\n            if is terminal \\n                discounted reward    \\n            discounted reward   reward   self gamma   discounted reward\\n            rewards insert    discounted reward \\n\\n          Normalize rewards\\n        rewards   torch tensor rewards  to device \\n        rewards  rewards   rewards mean     rewards std      e   \\n\\n          convert list to tensor\\n        old states   torch squeeze torch stack memory states  to device   detach  \\n        old actions   torch squeeze torch stack memory actions  to device   detach  \\n        old logprobs   torch squeeze torch stack memory logprobs   to device  detach  \\n\\n          Train policy for K epochs  sampling and updating\\n        for   in range self K epochs  \\n              Evaluate old actions and values using current policy\\n            logprobs  state values  dist entropy   self policy evaluate old states  old actions \\n\\n              Importance ratio  p q\\n            ratios   torch exp logprobs   old logprobs detach   \\n\\n              Advantages\\n            advantages   rewards   state values detach    old states  rewards   old states  valueevaluated by current policy \\n\\n              Actor loss using Surrogate loss\\n            surr    ratios   advantages\\n            surr    torch clamp ratios      self eps clip      self eps clip  advantages\\n            actor loss     torch min surr   surr  \\n\\n              Critic loss  critic loss   entropy\\n            critic loss         self MSE loss rewards  state values           dist entropy\\n\\n              Total loss\\n            loss   actor loss   critic loss\\n\\n              Backward gradients\\n            self optimizer zero grad  \\n            loss mean   backward  \\n            self optimizer step  \\n\\n          Copy new weights to old policy\\n        self old policy load state dict self policy state dict   \\n\\n\\n\\n    def update self  memory  \\n          Monte Carlo estimation of rewards\\n        rewards     \\n        discounted reward    \\n        for reward  is terminal in zip reversed memory rewards   reversed memory is terminals   \\n            if is terminal \\n                discounted reward    \\n            discounted reward   reward   self gamma   discounted reward\\n            rewards insert    discounted reward \\n\\n          Normalize rewards\\n        rewards   torch tensor rewards  to device \\n        rewards    rewards   rewards mean     rewards std    e   \\n\\n          convert list to tensor\\n        old states   torch squeeze torch stack memory states  to device   detach  \\n        old actions   torch squeeze torch stack memory actions  to device   detach  \\n        old logprobs   torch squeeze torch stack memory logprobs   to device  detach  \\n\\n          Train policy for K epochs  sampling and updating\\n        for   in range self K epochs  \\n              Evaluate old actions and values using current policy\\n            logprobs  state values  dist entropy   self policy evaluate old states  old actions \\n\\n              Importance ratio  p q\\n            ratios   torch exp logprobs   old logprobs detach   \\n\\n              Advantages\\n            advantages   rewards   state values detach  \\n\\n              Actor loss using Surrogate loss\\n            surr    ratios   advantages\\n            surr    torch clamp ratios      self eps clip      self eps clip  advantages\\n            actor loss     torch min surr   surr  \\n\\n              Critic loss  critic loss   entropy\\n            critic loss         self MSE loss rewards  state values           dist entropy\\n\\n              Total loss\\n            loss   actor loss   critic loss\\n\\n              Backward gradients\\n            self optimizer zero grad  \\n            loss mean   backward  \\n            self optimizer step  \\n\\n          Copy new weights to old policy\\n        self old policy load state dict self policy state dict   \\n\\n\\n\\n    def update self  memory  \\n        rewards     \\n        discounted reward    \\n        for reward  is terminal in zip reversed memory rewards   reversed memory is terminals   \\n            if is terminal \\n                discounted reward    \\n            discounted reward   reward  self gamma   discounted reward \\n            rewards insert    discounted reward \\n\\n          Normalizing the rewards \\n        rewards   torch tensor rewards  to device \\n        rewards  rewards   rewards mean     rewards std      e   \\n\\n          convert list to tensor\\n        old states   torch stack memory states  to device  detach  \\n        old actions   torch stack memory actions  to device  detach  \\n        old logprobs   torch stack memory logprobs  to device  detach  \\n\\n          Optimize policy for K epochs \\n        for   in range self K epochs  \\n              Evaluating old actions and values  \\n            logprobs  state values  dist entropy   self policy evaluate old states  old actions \\n\\n              Finding the ratiopi theta   pi theta  old  \\n            ratios   torch exp logprobs   old logprobs detach   \\n\\n              Finding Surrogate Loss \\n            advantages   rewards   state values detach  \\n            surr    ratios   advantages\\n            surr    torch clamp ratios      self eps clip      self eps clip  advantages\\n            loss    torch min surr   surr         self MseLoss state values  rewards         dist entropy\\n\\n              take gradient step\\n            self optimizer zero grad  \\n            loss mean   backward  \\n            self optimizer step  \\n\\n          Copy new weights into old policy \\n        self policy old load state dict self policy state dict   \\n\\n\\n\\n    def update self  memory     \\n          Monte Carlo estimate of state rewards \\n        rewards     \\n        discounted reward    \\n        for reward  is terminal in zip reversed memory rewards   reversed memory is terminals   \\n            if is terminal \\n                discounted reward    \\n                    \\n            discounted reward   reward  self gamma   discounted reward \\n                    \\n            rewards insert    discounted reward \\n        \\n          Normalizing the rewards \\n        rewards   torch tensor rewards  dtype torch float    to device \\n        rewards    rewards   rewards mean     rewards std    e   \\n        \\n          convert list to tensor\\n        old states   torch stack memory states  to device  detach  \\n        old actions   torch stack memory actions  to device  detach  \\n        old logprobs   torch stack memory logprobs  to device  detach  \\n        \\n          Optimize policy for K epochs \\n        for   in range self K epochs  \\n              Evaluating old actions and values  \\n            logprobs  state values  dist entropy   self policy evaluate old states  old actions \\n            \\n              Finding the ratio  pi theta   pi theta  old  \\n            ratios   torch exp logprobs   old logprobs detach   \\n                \\n              Finding Surrogate Loss \\n            advantages   rewards   state values detach  \\n            surr    ratios   advantages\\n            surr    torch clamp ratios    self eps clip    self eps clip    advantages\\n            loss    torch min surr   surr         self MseLoss state values  rewards         dist entropy\\n            \\n              take gradient step\\n            self optimizer zero grad  \\n            loss mean   backward  \\n            self optimizer step  \\n        \\n          Copy new weights into old policy \\n        self policy old load state dict self policy state dict   \\n        \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractedCode[579]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for word, tag in tagged_tokens:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            lemmatized_tokens.append(word)\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wn_tag))\n",
    "    return('  '.join(lemmatized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(extractedCode)):\n",
    "    extractedCode[i] = lemmatize_text(extractedCode[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "allBucketsInOne = lemmatize_text(allBucketsInOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def  update  self  memory  Monte  Carlo  estimate  of  reward  reward  discount  reward  for  reward  be  terminal  in  zip  reverse  memory  reward  reverse  memory  be  terminal  if  be  terminal  discount  reward  discount  reward  reward  self  gamma  discount  reward  reward  insert  discount  reward  Normalizing  the  reward  reward  torch  tensor  reward  to  device  reward  reward  reward  mean  reward  std  e  convert  list  to  tensor  old  state  torch  squeeze  torch  stack  memory  state  to  device  detach  old  action  torch  squeeze  torch  stack  memory  action  to  device  detach  old  logprobs  torch  squeeze  torch  stack  memory  logprobs  to  device  detach  Optimize  policy  for  K  epoch  for  in  range  self  K  epochs  Evaluating  old  action  and  value  logprobs  state  value  dist  entropy  self  policy  evaluate  old  state  old  action  Finding  the  ratio  pi  theta  pi  theta  old  ratio  torch  exp  logprobs  old  logprobs  detach  Finding  Surrogate  Loss  advantage  reward  state  value  detach  surr  ratio  advantage  surr  torch  clamp  ratio  self  eps  clip  self  eps  clip  advantage  loss  torch  min  surr  surr  self  MseLoss  state  value  reward  dist  entropy  take  gradient  step  self  optimizer  zero  grad  loss  mean  backward  self  optimizer  step  Copy  new  weight  into  old  policy  self  policy  old  load  state  dict  self  policy  state  dict  def  update  self  memory  Monte  Carlo  estimation  of  reward  reward  discount  reward  for  reward  be  terminal  in  zip  reverse  memory  reward  reverse  memory  be  terminal  if  be  terminal  discount  reward  discount  reward  reward  self  gamma  discount  reward  reward  insert  discount  reward  Normalize  reward  reward  torch  tensor  reward  to  device  reward  reward  reward  mean  reward  std  e  convert  list  to  tensor  old  state  torch  squeeze  torch  stack  memory  state  to  device  detach  old  action  torch  squeeze  torch  stack  memory  action  to  device  detach  old  logprobs  torch  squeeze  torch  stack  memory  logprobs  to  device  detach  Train  policy  for  K  epoch  sampling  and  update  for  in  range  self  K  epoch  Evaluate  old  action  and  value  use  current  policy  logprobs  state  value  dist  entropy  self  policy  evaluate  old  state  old  action  Importance  ratio  p  q  ratio  torch  exp  logprobs  old  logprobs  detach  Advantages  advantage  reward  state  value  detach  old  state  reward  old  state  valueevaluated  by  current  policy  Actor  loss  use  Surrogate  loss  surr  ratio  advantage  surr  torch  clamp  ratio  self  eps  clip  self  eps  clip  advantage  actor  loss  torch  min  surr  surr  Critic  loss  critic  loss  entropy  critic  loss  self  MSE  loss  reward  state  value  dist  entropy  Total  loss  loss  actor  loss  critic  loss  Backward  gradients  self  optimizer  zero  grad  loss  mean  backward  self  optimizer  step  Copy  new  weight  to  old  policy  self  old  policy  load  state  dict  self  policy  state  dict  def  update  self  memory  Monte  Carlo  estimation  of  reward  reward  discount  reward  for  reward  be  terminal  in  zip  reverse  memory  reward  reverse  memory  be  terminal  if  be  terminal  discount  reward  discount  reward  reward  self  gamma  discount  reward  reward  insert  discount  reward  Normalize  reward  reward  torch  tensor  reward  to  device  reward  reward  reward  mean  reward  std  e  convert  list  to  tensor  old  state  torch  squeeze  torch  stack  memory  state  to  device  detach  old  action  torch  squeeze  torch  stack  memory  action  to  device  detach  old  logprobs  torch  squeeze  torch  stack  memory  logprobs  to  device  detach  Train  policy  for  K  epoch  sampling  and  update  for  in  range  self  K  epoch  Evaluate  old  action  and  value  use  current  policy  logprobs  state  value  dist  entropy  self  policy  evaluate  old  state  old  action  Importance  ratio  p  q  ratio  torch  exp  logprobs  old  logprobs  detach  Advantages  advantage  reward  state  value  detach  Actor  loss  use  Surrogate  loss  surr  ratio  advantage  surr  torch  clamp  ratio  self  eps  clip  self  eps  clip  advantage  actor  loss  torch  min  surr  surr  Critic  loss  critic  loss  entropy  critic  loss  self  MSE  loss  reward  state  value  dist  entropy  Total  loss  loss  actor  loss  critic  loss  Backward  gradients  self  optimizer  zero  grad  loss  mean  backward  self  optimizer  step  Copy  new  weight  to  old  policy  self  old  policy  load  state  dict  self  policy  state  dict  def  update  self  memory  reward  discount  reward  for  reward  be  terminal  in  zip  reverse  memory  reward  reverse  memory  be  terminal  if  be  terminal  discount  reward  discount  reward  reward  self  gamma  discount  reward  reward  insert  discount  reward  Normalizing  the  reward  reward  torch  tensor  reward  to  device  reward  reward  reward  mean  reward  std  e  convert  list  to  tensor  old  state  torch  stack  memory  state  to  device  detach  old  action  torch  stack  memory  action  to  device  detach  old  logprobs  torch  stack  memory  logprobs  to  device  detach  Optimize  policy  for  K  epoch  for  in  range  self  K  epochs  Evaluating  old  action  and  value  logprobs  state  value  dist  entropy  self  policy  evaluate  old  state  old  action  Finding  the  ratiopi  theta  pi  theta  old  ratio  torch  exp  logprobs  old  logprobs  detach  Finding  Surrogate  Loss  advantage  reward  state  value  detach  surr  ratio  advantage  surr  torch  clamp  ratio  self  eps  clip  self  eps  clip  advantage  loss  torch  min  surr  surr  self  MseLoss  state  value  reward  dist  entropy  take  gradient  step  self  optimizer  zero  grad  loss  mean  backward  self  optimizer  step  Copy  new  weight  into  old  policy  self  policy  old  load  state  dict  self  policy  state  dict  def  update  self  memory  Monte  Carlo  estimate  of  state  reward  reward  discount  reward  for  reward  be  terminal  in  zip  reverse  memory  reward  reverse  memory  be  terminal  if  be  terminal  discount  reward  discount  reward  reward  self  gamma  discount  reward  reward  insert  discount  reward  Normalizing  the  reward  reward  torch  tensor  reward  dtype  torch  float  to  device  reward  reward  reward  mean  reward  std  e  convert  list  to  tensor  old  state  torch  stack  memory  state  to  device  detach  old  action  torch  stack  memory  action  to  device  detach  old  logprobs  torch  stack  memory  logprobs  to  device  detach  Optimize  policy  for  K  epoch  for  in  range  self  K  epochs  Evaluating  old  action  and  value  logprobs  state  value  dist  entropy  self  policy  evaluate  old  state  old  action  Finding  the  ratio  pi  theta  pi  theta  old  ratio  torch  exp  logprobs  old  logprobs  detach  Finding  Surrogate  Loss  advantage  reward  state  value  detach  surr  ratio  advantage  surr  torch  clamp  ratio  self  eps  clip  self  eps  clip  advantage  loss  torch  min  surr  surr  self  MseLoss  state  value  reward  dist  entropy  take  gradient  step  self  optimizer  zero  grad  loss  mean  backward  self  optimizer  step  Copy  new  weight  into  old  policy  self  policy  old  load  state  dict  self  policy  state  dict'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractedCode[579]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import corpora\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem_text\n",
    "from gensim.utils import to_unicode\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import scipy.sparse\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    # Tokenize by splitting on non-alphanumeric characters\n",
    "    return re.split(r'\\W+', text)\n",
    "\n",
    "def preprocessForTopicModelling(prompts_text, run_simple_tokenize=False):\n",
    "    if run_simple_tokenize:\n",
    "        processed_data = [simple_tokenize(text.lower()) for text in prompts_text]\n",
    "    else:\n",
    "        processed_data = [preprocess_string(text, filters=[lambda x: x.lower(), remove_stopwords, stem_text]) for text in prompts_text]\n",
    "    dictionary = corpora.Dictionary(processed_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_data]\n",
    "\n",
    "    return dictionary, corpus, processed_data\n",
    "\n",
    "def get_sorted_topics(document_topic_dist):\n",
    "    \"\"\"Sort topics by their probability in descending order.\"\"\"\n",
    "    return [sorted(topics, key=lambda x: -x[1])[0] if topics else (None, 0) for topics in document_topic_dist]\n",
    "\n",
    "def get_topic_details(lda_model, topic_id, num_words=7):\n",
    "    \"\"\"Retrieve detailed words and their probabilities for a topic.\"\"\"\n",
    "    if topic_id is not None:\n",
    "        topic_words = lda_model.show_topic(topic_id, topn=num_words)\n",
    "        return [(word, prob) for word, prob in topic_words]\n",
    "    else:\n",
    "        return [\"No dominant topic or error in topic modeling\"]\n",
    "    \n",
    "def getTopicInfo(lda_model, document_topic_dist, num_words=7):\n",
    "    dominant_topic_info = pd.DataFrame()\n",
    "    sorted_topics = get_sorted_topics(document_topic_dist)\n",
    "    for i, (topic_id, prob) in enumerate(sorted_topics):\n",
    "        topic_info = get_topic_details(lda_model, topic_id, num_words)\n",
    "        dominant_topic_info = pd.concat([dominant_topic_info, pd.DataFrame({\"Topic ID\": [topic_id], \"Topic Probability\": [prob], \"Topic Words\": [topic_info]})], ignore_index=True)\n",
    "\n",
    "    all_topics_info = pd.DataFrame()\n",
    "    for topics in document_topic_dist:\n",
    "        if topics:  # Ensure topics is not empty\n",
    "            for topic_id, prob in topics:\n",
    "                topic_info = get_topic_details(lda_model, topic_id, num_words)\n",
    "                all_topics_info = pd.concat([all_topics_info, pd.DataFrame({\"Topic ID\": [topic_id], \"Topic Probability\": [prob], \"Topic Words\": [topic_info]})], ignore_index=True)\n",
    "    return all_topics_info, dominant_topic_info\n",
    "\n",
    "\n",
    "def runTopicModelling(processed_data, dictionary, corpus, optimal_num_topics=5):\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=optimal_num_topics, passes=10)\n",
    "    document_topic_dist = [lda_model.get_document_topics(doc, minimum_probability=0) for doc in corpus]\n",
    "\n",
    "    all_topics_info, dominant_topic_info = getTopicInfo(lda_model, document_topic_dist, num_words=7)\n",
    "    return all_topics_info, dominant_topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find optimal number of topics to use in LDA by using coherence score\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step = 6):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "start, limit, step = 2, 40, 6\n",
    "dictionary, corpus, processed_data = preprocessForTopicModelling(extractedCode)\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_data, start=start, limit=limit, step=step)\n",
    "\n",
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give max coherence score from the graph\n",
    "max_coherence_score = max(coherence_values)\n",
    "max_coherence_score_index = coherence_values.index(max_coherence_score)\n",
    "optimal_num_topics = x[max_coherence_score_index]\n",
    "optimal_num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, corpus, processed_data = preprocessForTopicModelling(extractedCode , run_simple_tokenize=True)\n",
    "all_topics_info, dominant_topic_info = runTopicModelling(processed_data, dictionary, corpus, optimal_num_topics=optimal_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic ID</th>\n",
       "      <th>Topic Probability</th>\n",
       "      <th>Topic Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>[(self, 0.19366688), (nn, 0.06379178), (d, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.166266</td>\n",
       "      <td>[(image, 0.036314674), (path, 0.03450716), (im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>[(m, 0.019415414), (data, 0.016313631), (if, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.122062</td>\n",
       "      <td>[(x, 0.123805344), (self, 0.04888734), (out, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.211969</td>\n",
       "      <td>[(loss, 0.06707748), (batch, 0.028360616), (la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26611</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>[(x, 0.123805344), (self, 0.04888734), (out, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26612</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>[(loss, 0.06707748), (batch, 0.028360616), (la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26613</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>[(tensor, 0.026480533), (image, 0.025498932), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26614</th>\n",
       "      <td>6</td>\n",
       "      <td>0.009328</td>\n",
       "      <td>[(i, 0.036688775), (in, 0.02817), (for, 0.0221...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26615</th>\n",
       "      <td>7</td>\n",
       "      <td>0.080035</td>\n",
       "      <td>[(model, 0.037474528), (args, 0.02895278), (tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26616 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic ID  Topic Probability  \\\n",
       "0             0           0.000339   \n",
       "1             1           0.166266   \n",
       "2             2           0.000339   \n",
       "3             3           0.122062   \n",
       "4             4           0.211969   \n",
       "...         ...                ...   \n",
       "26611         3           0.000599   \n",
       "26612         4           0.000599   \n",
       "26613         5           0.000599   \n",
       "26614         6           0.009328   \n",
       "26615         7           0.080035   \n",
       "\n",
       "                                             Topic Words  \n",
       "0      [(self, 0.19366688), (nn, 0.06379178), (d, 0.0...  \n",
       "1      [(image, 0.036314674), (path, 0.03450716), (im...  \n",
       "2      [(m, 0.019415414), (data, 0.016313631), (if, 0...  \n",
       "3      [(x, 0.123805344), (self, 0.04888734), (out, 0...  \n",
       "4      [(loss, 0.06707748), (batch, 0.028360616), (la...  \n",
       "...                                                  ...  \n",
       "26611  [(x, 0.123805344), (self, 0.04888734), (out, 0...  \n",
       "26612  [(loss, 0.06707748), (batch, 0.028360616), (la...  \n",
       "26613  [(tensor, 0.026480533), (image, 0.025498932), ...  \n",
       "26614  [(i, 0.036688775), (in, 0.02817), (for, 0.0221...  \n",
       "26615  [(model, 0.037474528), (args, 0.02895278), (tr...  \n",
       "\n",
       "[26616 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topics_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic ID</th>\n",
       "      <th>Topic Probability</th>\n",
       "      <th>Topic Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.472178</td>\n",
       "      <td>[(tensor, 0.026480533), (image, 0.025498932), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.854864</td>\n",
       "      <td>[(model, 0.037474528), (args, 0.02895278), (tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.524535</td>\n",
       "      <td>[(m, 0.019415414), (data, 0.016313631), (if, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.520338</td>\n",
       "      <td>[(m, 0.019415414), (data, 0.016313631), (if, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.675147</td>\n",
       "      <td>[(x, 0.123805344), (self, 0.04888734), (out, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3322</th>\n",
       "      <td>0</td>\n",
       "      <td>0.910444</td>\n",
       "      <td>[(self, 0.19366688), (nn, 0.06379178), (d, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>2</td>\n",
       "      <td>0.729933</td>\n",
       "      <td>[(m, 0.019415414), (data, 0.016313631), (if, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3324</th>\n",
       "      <td>1</td>\n",
       "      <td>0.589365</td>\n",
       "      <td>[(image, 0.036314674), (path, 0.03450716), (im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3325</th>\n",
       "      <td>0</td>\n",
       "      <td>0.846604</td>\n",
       "      <td>[(self, 0.19366688), (nn, 0.06379178), (d, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>0</td>\n",
       "      <td>0.907643</td>\n",
       "      <td>[(self, 0.19366688), (nn, 0.06379178), (d, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3327 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic ID  Topic Probability  \\\n",
       "0            5           0.472178   \n",
       "1            7           0.854864   \n",
       "2            2           0.524535   \n",
       "3            2           0.520338   \n",
       "4            3           0.675147   \n",
       "...        ...                ...   \n",
       "3322         0           0.910444   \n",
       "3323         2           0.729933   \n",
       "3324         1           0.589365   \n",
       "3325         0           0.846604   \n",
       "3326         0           0.907643   \n",
       "\n",
       "                                            Topic Words  \n",
       "0     [(tensor, 0.026480533), (image, 0.025498932), ...  \n",
       "1     [(model, 0.037474528), (args, 0.02895278), (tr...  \n",
       "2     [(m, 0.019415414), (data, 0.016313631), (if, 0...  \n",
       "3     [(m, 0.019415414), (data, 0.016313631), (if, 0...  \n",
       "4     [(x, 0.123805344), (self, 0.04888734), (out, 0...  \n",
       "...                                                 ...  \n",
       "3322  [(self, 0.19366688), (nn, 0.06379178), (d, 0.0...  \n",
       "3323  [(m, 0.019415414), (data, 0.016313631), (if, 0...  \n",
       "3324  [(image, 0.036314674), (path, 0.03450716), (im...  \n",
       "3325  [(self, 0.19366688), (nn, 0.06379178), (d, 0.0...  \n",
       "3326  [(self, 0.19366688), (nn, 0.06379178), (d, 0.0...  \n",
       "\n",
       "[3327 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics_info.to_csv(\"../../analysis/RQ3/all_topics_info.csv\", index=False)\n",
    "dominant_topic_info.to_csv(\"../../analysis/RQ3/dominant_topic_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Modelling on one entire document\n",
    "- randomly pick one pre processed snippet from each class/bucket\n",
    "- concatenate them into one document \n",
    "- apply topic modelling on that big single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessForTopicModelling([allBucketsInOne], run_simple_tokenize=True)\n",
    "\n",
    "dictionary, corpus, processed_data = preprocessForTopicModelling([allBucketsInOne], run_simple_tokenize=True)\n",
    "all_topics_info, dominant_topic_info = runTopicModelling(processed_data, dictionary, corpus, optimal_num_topics=optimal_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic ID</th>\n",
       "      <th>Topic Probability</th>\n",
       "      <th>Topic Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.285066e-07</td>\n",
       "      <td>[(self, 0.016131181), (size, 0.0096446015), (i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.285783e-07</td>\n",
       "      <td>[(self, 0.022960866), (size, 0.01753713), (los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.283841e-07</td>\n",
       "      <td>[(self, 0.0056956187), (size, 0.003719733), (m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.999984e-01</td>\n",
       "      <td>[(self, 0.034579303), (size, 0.015174575), (mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.283922e-07</td>\n",
       "      <td>[(self, 0.008165202), (size, 0.003073128), (nn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.284314e-07</td>\n",
       "      <td>[(self, 0.007421445), (size, 0.0059193335), (i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2.284582e-07</td>\n",
       "      <td>[(self, 0.014206006), (model, 0.0058023576), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.286663e-07</td>\n",
       "      <td>[(self, 0.028471768), (size, 0.01471322), (if,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic ID  Topic Probability  \\\n",
       "0         0       2.285066e-07   \n",
       "1         1       2.285783e-07   \n",
       "2         2       2.283841e-07   \n",
       "3         3       9.999984e-01   \n",
       "4         4       2.283922e-07   \n",
       "5         5       2.284314e-07   \n",
       "6         6       2.284582e-07   \n",
       "7         7       2.286663e-07   \n",
       "\n",
       "                                         Topic Words  \n",
       "0  [(self, 0.016131181), (size, 0.0096446015), (i...  \n",
       "1  [(self, 0.022960866), (size, 0.01753713), (los...  \n",
       "2  [(self, 0.0056956187), (size, 0.003719733), (m...  \n",
       "3  [(self, 0.034579303), (size, 0.015174575), (mo...  \n",
       "4  [(self, 0.008165202), (size, 0.003073128), (nn...  \n",
       "5  [(self, 0.007421445), (size, 0.0059193335), (i...  \n",
       "6  [(self, 0.014206006), (model, 0.0058023576), (...  \n",
       "7  [(self, 0.028471768), (size, 0.01471322), (if,...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topics_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic ID</th>\n",
       "      <th>Topic Probability</th>\n",
       "      <th>Topic Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>[(self, 0.034579303), (size, 0.015174575), (mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic ID  Topic Probability  \\\n",
       "0         3           0.999998   \n",
       "\n",
       "                                         Topic Words  \n",
       "0  [(self, 0.034579303), (size, 0.015174575), (mo...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics_info.to_csv(\"../../analysis/RQ3/all_topics_info_allBuckets.csv\", index=False)\n",
    "dominant_topic_info.to_csv(\"../../analysis/RQ3/dominant_topic_info_allBuckets.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
